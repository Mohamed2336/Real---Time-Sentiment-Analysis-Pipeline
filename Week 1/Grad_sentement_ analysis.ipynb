{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e240b369",
   "metadata": {},
   "source": [
    "# Importing Python Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d8d3f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ff7c7",
   "metadata": {},
   "source": [
    "## loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2dceb3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding = \"ISO-8859-1\", engine=\"python\")\n",
    "data.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bad7e8c",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "acb47d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label        time                          date     query       username  \\\n",
       "0      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n",
       "1      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n",
       "2      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n",
       "3      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n",
       "4      0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n",
       "\n",
       "                                                text  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a65d5281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label        time                          date     query  \\\n",
       "1599994      4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599995      4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996      4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997      4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998      4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                username                                               text  \n",
       "1599994  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599995      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599996           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599997     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599998   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "475b83e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'time', 'date', 'query', 'username', 'text'], dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ed43923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenght of data is 1599999\n"
     ]
    }
   ],
   "source": [
    "# length of data \n",
    "print('lenght of data is', len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4fa82caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599999, 6)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of data \n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "57998fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599999 entries, 0 to 1599998\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count    Dtype \n",
      "---  ------    --------------    ----- \n",
      " 0   label     1599999 non-null  int64 \n",
      " 1   time      1599999 non-null  int64 \n",
      " 2   date      1599999 non-null  object\n",
      " 3   query     1599999 non-null  object\n",
      " 4   username  1599999 non-null  object\n",
      " 5   text      1599999 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "#data information \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65c20c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking null values \n",
    "np.sum(data.isnull().any(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e73e5b8",
   "metadata": {},
   "source": [
    "# Data preprossing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0e7a9cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the text and label coloumn\n",
    "\n",
    "data=data[['text','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2eac65f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Assigning 1 to Positive sentment 4\n",
    "data['label'][data['label']==4]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "185ff929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b6d0e47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating positive and negative tweets\n",
    "data_pos = data[data['label'] == 1]\n",
    "data_neg = data[data['label'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "54d7f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking one fourth data so we can run on our machine easily\n",
    "\n",
    "data_pos = data_pos.iloc[:int(20000)]\n",
    "data_neg = data_neg.iloc[:int(20000)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c81bb8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining positive and negative tweets\n",
    "\n",
    "data = pd.concat([data_pos, data_neg])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9c4617cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making statement text in lower case\n",
    "\n",
    "data['text']=data['text'].str.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ee233a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999         i love @health4uandpets u guys r the best!! \n",
       "800000    im meeting up with one of my besties tonight! ...\n",
       "800001    @darealsunisakim thanks for the twitter add, s...\n",
       "800002    being sick can be really cheap when it hurts t...\n",
       "800003      @lovesbrooklyn2 he has that effect on everyone \n",
       "                                ...                        \n",
       "19995                             one more day of holidays \n",
       "19996     feeling so down right now .. i hate you damn h...\n",
       "19997     geez,i hv to read the whole book of personalit...\n",
       "19998     i threw my sign at donnie and he bent over to ...\n",
       "19999     @heather2711 good thing i didn't find any then...\n",
       "Name: text, Length: 40000, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f4f5eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning and removing Stop words of english\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8ec98a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\", \".join(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1f24e679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999                love @health4uandpets u guys r best!!\n",
       "800000    im meeting one besties tonight! cant wait!! - ...\n",
       "800001    @darealsunisakim thanks twitter add, sunisa! g...\n",
       "800002    sick really cheap hurts much eat real food plu...\n",
       "800003                      @lovesbrooklyn2 effect everyone\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning and removing the above stop words list from the tweet text\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "data['text'] = data['text'].apply(lambda text: cleaning_stopwords(text))\n",
    "data['text'].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "666a6032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning and removing punctuations\n",
    "\n",
    "\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "33a78308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999                   love health4uandpets u guys r best\n",
       "800000    im meeting one besties tonight cant wait  girl...\n",
       "800001    darealsunisakim thanks twitter add sunisa got ...\n",
       "800002    sick really cheap hurts much eat real food plu...\n",
       "800003                       lovesbrooklyn2 effect everyone\n",
       "                                ...                        \n",
       "19995                                      one day holidays\n",
       "19996                      feeling right  hate damn humprey\n",
       "19997     geezi hv read whole book personality types emb...\n",
       "19998      threw sign donnie bent get thingee made sad face\n",
       "19999     heather2711 good thing find none ones like com...\n",
       "Name: text, Length: 40000, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text']= data['text'].apply(lambda x: cleaning_punctuations(x))\n",
    "data['text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "89926fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#leaning and removing repeating characters\n",
    "\n",
    "def cleaning_repeating_char(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "19643161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999                   love health4uandpets u guys r best\n",
       "800000    im meting one besties tonight cant wait girl talk\n",
       "800001    darealsunisakim thanks twiter ad sunisa got me...\n",
       "800002    sick realy cheap hurts much eat real fod plus ...\n",
       "800003                         lovesbroklyn2 efect everyone\n",
       "                                ...                        \n",
       "19995                                      one day holidays\n",
       "19996                        feling right hate damn humprey\n",
       "19997     gezi hv read whole bok personality types embar...\n",
       "19998        threw sign donie bent get thinge made sad face\n",
       "19999     heather271 god thing find none ones like come ...\n",
       "Name: text, Length: 40000, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].apply(lambda x: cleaning_repeating_char(x))\n",
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "06667510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning and removing email\n",
    "\n",
    "def cleaning_email(data):\n",
    "    return re.sub('@[^\\s]+', ' ', data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3ae5be1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999                   love health4uandpets u guys r best\n",
       "800000    im meting one besties tonight cant wait girl talk\n",
       "800001    darealsunisakim thanks twiter ad sunisa got me...\n",
       "800002    sick realy cheap hurts much eat real fod plus ...\n",
       "800003                         lovesbroklyn2 efect everyone\n",
       "                                ...                        \n",
       "19995                                      one day holidays\n",
       "19996                        feling right hate damn humprey\n",
       "19997     gezi hv read whole bok personality types embar...\n",
       "19998        threw sign donie bent get thinge made sad face\n",
       "19999     heather271 god thing find none ones like come ...\n",
       "Name: text, Length: 40000, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text']= data['text'].apply(lambda x: cleaning_email(x))\n",
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "024522b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning and removing URL's\n",
    "\n",
    "def cleaning_URLs(data):\n",
    "    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1706467a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999                   love health4uandpets u guys r best\n",
       "800000    im meting one besties tonight cant wait girl talk\n",
       "800001    darealsunisakim thanks twiter ad sunisa got me...\n",
       "800002    sick realy cheap hurts much eat real fod plus ...\n",
       "800003                         lovesbroklyn2 efect everyone\n",
       "                                ...                        \n",
       "19995                                      one day holidays\n",
       "19996                        feling right hate damn humprey\n",
       "19997     gezi hv read whole bok personality types embar...\n",
       "19998        threw sign donie bent get thinge made sad face\n",
       "19999     heather271 god thing find none ones like come ...\n",
       "Name: text, Length: 40000, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].apply(lambda x: cleaning_URLs(x))\n",
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b8aa86fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning and removing Numeric numbers\n",
    "\n",
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7e49482c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19995                                     one day holidays\n",
       "19996                       feling right hate damn humprey\n",
       "19997    gezi hv read whole bok personality types embar...\n",
       "19998       threw sign donie bent get thinge made sad face\n",
       "19999    heather god thing find none ones like come siz...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].apply(lambda x: cleaning_numbers(x))\n",
    "data['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "93a110cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting tokenization of tweet text\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data['text'] = data['text'].apply(tokenizer.tokenize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "542e0cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799999             [love, healthuandpets, u, guys, r, best]\n",
       "800000    [im, meting, one, besties, tonight, cant, wait...\n",
       "800001    [darealsunisakim, thanks, twiter, ad, sunisa, ...\n",
       "800002    [sick, realy, cheap, hurts, much, eat, real, f...\n",
       "800003                      [lovesbroklyn, efect, everyone]\n",
       "                                ...                        \n",
       "19995                                  [one, day, holidays]\n",
       "19996                  [feling, right, hate, damn, humprey]\n",
       "19997     [gezi, hv, read, whole, bok, personality, type...\n",
       "19998     [threw, sign, donie, bent, get, thinge, made, ...\n",
       "19999     [heather, god, thing, find, none, ones, like, ...\n",
       "Name: text, Length: 40000, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d83bdc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799999              [love, healthuandpets, u, guy, r, best]\n",
      "800000    [im, meting, one, besties, tonight, cant, wait...\n",
      "800001    [darealsunisakim, thanks, twiter, ad, sunisa, ...\n",
      "800002    [sick, realy, cheap, hurt, much, eat, real, fo...\n",
      "800003                      [lovesbroklyn, efect, everyone]\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Initialize Lemmatizer\n",
    "lm = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Function to apply Lemmatization\n",
    "def lemmatizer_on_text(data):\n",
    "    return [lm.lemmatize(word) for word in data]\n",
    "\n",
    "# Applying Lemmatization to the text\n",
    "data['text'] = data['text'].apply(lambda x: lemmatizer_on_text(x))\n",
    "\n",
    "# Display the first 5 results after applying Lemmatization\n",
    "print(data['text'].head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "814374f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating input feature and label\n",
    "\n",
    "X=data.text\n",
    "y=data.label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c2113540",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the input features for training\n",
    "max_len = 500  # Maximum number of words per text\n",
    "tok = Tokenizer(num_words=2000)  # Creating a word dictionary with a maximum of 2000 words\n",
    "tok.fit_on_texts(X)  # Building the word index based on texts in X\n",
    "sequences = tok.texts_to_sequences(X)  # Converting texts to sequences of numbers\n",
    "sequences_matrix = sequence.pad_sequences(sequences, maxlen=max_len)  # Padding sequences to ensure uniform length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "62adc952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 500)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e452fc",
   "metadata": {},
   "source": [
    "## Separating the 70% data for training data and 30% for testing data\n",
    "\n",
    "As we prepared all the tweets, now we are separating/splitting the tweets into training data and testing data.\n",
    "\n",
    "    70% tweets will be used in the training\n",
    "    30% tweets will be used to test the performance of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f66f3563",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(sequences_matrix, y, test_size=0.3, random_state=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
